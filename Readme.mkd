Le projet consiste à concevoir une application web qui permettra aux utilisateurs de capturer des vidéos de leurs gestes en utilisant la caméra de leur appareil. Une fois la vidéo capturée, celle-ci sera divisée en plusieurs images. Ensuite, chaque signe apparaissant dans chaque image sera traduit. Ainsi, une succession de signes donnera une succession de lettres, formant ainsi un mot. L’application traduira automatiquement ces mots en texte dans les langues cibles, à savoir le français, l’anglais et l’arabe. Ce système utilisera des algorithmes de machine learning pour détecter et interpréter les différents signes.

À l’origine, notre ambition était de traduire les signes en temps réel. Cependant, nous avons rapidement réalisé que cette tâche était bien trop complexe compte tenu du laps de temps limité dont nous disposions.